{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 教程\n",
    "## 配套教材《深度学习入门——基于python的理论与实现》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 的两个重要特性 自动求导/微分 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# N是批量大小; D_in是输入维度;\n",
    "# 49/5000 H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "# 创建随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # 前向传递：计算预测值y\n",
    "    h = np.dot(x,w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = np.dot(h_relu,w2)\n",
    "    # 计算和打印损失loss\n",
    "    loss = np.sum(np.square(y_pred - y))#MSE 误差\n",
    "    print(t, loss)\n",
    "    \n",
    "    # 反向传播，计算w1和w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = np.dot(h_relu.T,grad_y_pred)\n",
    "    grad_h_relu = np.dot(grad_y_pred,w2.T)\n",
    "    \n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    \n",
    "    grad_w1 = np.dot(x.T,grad_h)\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n",
    "# N是批量大小; D_in是输入维度;\n",
    "# H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # 前向传递：计算预测y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    # 计算和打印损失\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "    # Backprop计算w1和w2相对于损耗的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n",
    "# N是批量大小; D_in是输入维度;\n",
    "# H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "# 创建随机Tensors以保持输入和输出。\n",
    "# 设置requires_grad = False表示我们不需要计算梯度\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "# 为权重创建随机Tensors。\n",
    "# 设置requires_grad = True表示我们想要计算渐变\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：使用tensors上的操作计算预测值y; \n",
    "      # 由于w1和w2有requires_grad=True，涉及这些张量的操作将让PyTorch构建计算图，\n",
    "    # 从而允许自动计算梯度。由于我们不再手工实现反向传播，所以不需要保留中间值的引用。\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    # 使用Tensors上的操作计算和打印丢失。\n",
    "    # loss是一个形状为()的张量\n",
    "    # loss.item() 得到这个张量对应的python数值\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    loss.backward()\n",
    "    # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段\n",
    "    # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # 反向传播后手动将梯度设置为零\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，\n",
    "    并完成张量的正向和反向传播。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        在正向传播中，我们接收到一个上下文对象和一个包含输入的张量；\n",
    "        我们必须返回一个包含输出的张量，\n",
    "        并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收到上下文对象和一个张量，\n",
    "        其包含了相对于正向传播过程中产生的输出的损失的梯度。\n",
    "        我们可以从上下文对象中检索缓存的数据，\n",
    "        并且必须计算并返回与正向传播的输入相关的损失的梯度。\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "# 产生随机权重的张量\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 正向传播：使用张量上的操作来计算输出值y；\n",
    "    # 我们通过调用 MyReLU.apply 函数来使用自定义的ReLU\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "    # 计算并输出loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    # 使用autograd计算反向传播过程。\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        # 用梯度下降更新权重\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # 在反向传播之后手动清零梯度\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
